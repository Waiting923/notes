# 容器云环境部署
## 系统环境配置(Ubuntu20.04)
### hosts
```
#配置对应节点hosts解析
$ vim /etc/hosts
```
### apt源修改
```
#除正常源外需额外添加ceph17.2.6源
deb http://mirrors.yovole.tech/ceph/debian-quincy/ focal main

#更新key
$ curl http://mirrors.yovole.tech/ceph/keys/release.asc | apt-key add -
```
### dns
```
#配置对应区域dns-server地址
$ vim /etc/systemd/resolved.conf
[Resolve]
DNS=xxx.xxx.xxx.xxx

$ rm /etc/resolve.conf

$ ln -s /run/systemd/resolve/resolv.conf /etc/resolv.conf

$ systemctl restart systemd-resolved.service
```
### ulimit
```
$ vim /etc/security/limits.conf
* soft nproc 65535
* hard nproc 65535
* soft nofile 65535
* hard nofile 65535
root soft nproc 65535
root hard nproc 65535
root soft nofile 65535
root hard nofile 65535
```
### sysctl
```
$ vim /etc/sysctl.conf
net.ipv4.ip_nonlocal_bind = 1 #这个参数是给keepalive起vip用，只需要加到master节点
#现在调整了kube-proxy的结构，下面这个参数不需要添加了
net.ipv4.conf.eth2.arp_ignore=1 #这个参数是用来禁止非istio节点广播istio-ingressgw的external ip的mac地址，除istio节点外添加
```
### chrony
```
$ apt -y install chrony

# 配置对应区域ntp地址
$ vim /etc/chronyd
server xxx.xxx.xxx.xxx iburst
```
## sealos部署Super-k8s
### 离线安装sealos
```
#获取sealos-4.2离线包并解压
$ cp /root/deploy/sealos-4.2/sealos /usr/bin/sealos

$ sealos version
SealosVersion:
  buildDate: "2023-04-23T14:23:15Z"
  compiler: gc
  gitCommit: f696a621
  gitVersion: 4.2.0
  goVersion: go1.20.3
  platform: linux/amd64
```
### sealos生成配置文件
```
#指定对应组件版本，指定master,node节点ip,多节点用","分隔
$ sealos gen labring/kubernetes:v1.25.0 labring/helm:v3.8.2 labring/calico:v3.24.1 --masters xxx.xxx.xxx.xxx --nodes xxx.xxx.xxx.xxx >
 Clusterfile
```
### 添加calico配置
```
$ vim Clusterfile
---
apiVersion: apps.sealos.io/v1beta1
kind: Config
metadata:
  name: calico
spec:
  path: charts/calico/values.yaml
  strategy: merge
  data: |
    installation:
      enabled: true
      kubernetesProvider: ""
      calicoNetwork:
        ipPools:
        - blockSize: 26
          cidr: xxx.xxx.xxx.xxx/xx #指定calico的PodSubnet
          encapsulation: IPIP
          natOutgoing: Enabled
          nodeSelector: all()
        nodeAddressAutodetectionV4:
          interface: "eth0|team1.13" #指定calico对应物理网卡
```
### 修改集群pod/service地址段
```
$ vim Clusterfile
Networking:
  PodSubnet: xxx.xxx.xxx.xxx/xx
  ServiceSubnet: xxx.xxx.xxx.xxx/xx
```
### sealos部署
```
$ sealos apply -f Clusterfile

#执行以下命令重新部署
$ sealos reset
```
## Super-k8s集群调整
### containerd数据盘调整
```
$ mkdir /data

$ mkfs.xfs /dev/sdb

$ mount /dev/sdb /data

$ echo '/dev/sdb /data xfs defaults 0 0' >> /etc/fstab

$ vim /etc/containerd/config.toml
root = "/data/containerd"

$ systemctl restart containerd
```
### 共享存储挂载
```
$ apt -y install ceph-common-17.2.6

$ cp ceph.conf /etc/ceph/

$ cp ceph.client.csk.keyring /etc/ceph/

$ mkdir /etc/csk

$ mkdir /usr/share/kata-containers

$ mount -t ceph csk@.cfs_ssd=/volumes/csk/csk_config/ef07195b-2768-47b3-8b87-4bd0c82acff1 /etc/csk -o mon_addr=100.71.50.163:6789/100.71.50.165:6789/100.71.50.185:6789

$ mount -t ceph csk@.cfs_ssd=/volumes/csk/node_container_kata/af132c13-8e40-4297-a207-6e25080f85aa /usr/share/kata-containers -o mon_addr=100.71.50.163:6789/100.71.50.165:6789/100.71.50.185:6789

$ echo 'csk@.cfs_ssd=/volumes/csk/csk_config/ef07195b-2768-47b3-8b87-4bd0c82acff1 /etc/csk ceph mon_addr=100.71.50.163:6789/100.71.50.165:6789/100.71.50.185:6789,noatime,_netdev 0 0' >> /etc/fstab

$ echo 'csk@.cfs_ssd=/volumes/csk/node_container_kata/af132c13-8e40-4297-a207-6e25080f85aa /usr/share/kata-containers ceph mon_addr=100.71.50.163:6789/100.71.50.165:6789/100.71.50.185:6789,noatime,_netdev 0 0' >> /etc/fstab

$ rm /etc/containerd/certs.d

$ ln -s /etc/csk/containerd_certs.d/ ./certs.d
```
### 禁用image-cri-shim
```
$ vim tag.sh
#!/bin/bash
cmd1=$(crictl images | grep sealos.hub | grep calico |awk '{print $1}' | sed 's/sealos.hub:5000\///g')
for i in $cmd1
do
  tag1=$(crictl images | grep sealos.hub |grep calico | grep "$i " | awk '{print $2}')
  src1=sealos.hub:5000/$i:$tag1
  dest1=docker.io/$i:$tag1
  echo $src1 to $dest1
  ctr -n k8s.io i tag $src1 $dest1
done

cmd2=$(crictl images | grep sealos.hub |grep -v calico |grep -v lvscare |awk '{print $1}' | sed 's/sealos.hub:5000\///g')
for i in $cmd2
do
  tag2=$(crictl images | grep sealos.hub | grep "$i " | awk '{print $2}')
  src2=sealos.hub:5000/$i:$tag2
  dest2=registry.k8s.io/$i:$tag2
  echo $src2 to $dest2
  ctr -n k8s.io i tag $src2 $dest2
done

cmd3=$(crictl images | grep sealos.hub |grep lvscare |awk '{print $1}' | sed 's/sealos.hub:5000\///g')
if [[ -n $cmd3 ]]
then
  for i in $cmd3
    do
      tag3=$(crictl images | grep sealos.hub | grep "$i " | awk '{print $2}')
      src3=sealos.hub:5000/$i:$tag3
      dest3=ghcr.io/labring/$i:$tag3
      echo $src3 to $dest3
      ctr -n k8s.io i tag $src3 $dest3
    done

#去掉kubelet以下参数
$ vim /var/lib/kubelet/kubelet-flags.env
--image-service-endpoint /var/run/image-cri-shim.sock

$ systemctl restart kubelet

$ systemctl stop image-cri-shim

$ systemctl disable image-cri-shim

$ vim /etc/crictl.yaml
#image-endpoint: unix:///var/run/image-cri-shim.sock
```
### 替换worker节点kubelet(编译好支持kata runtime probe的kubelet二进制文件)
```
$ systemctl stop kubelet

$ cp kubelet-csk /usr/bin/kubelet

$ systemctl start kubelet

$ kubectl get nodes
NAME                        STATUS   ROLES     AGE    VERSION
cn-east-2a-csk-gpu-149      Ready    worker    15d    v1.25.0-csk
```
## Kuryr-kubernetes部署
### kuryr-cni节点准备(源码安装neutron-ovs-agent)
```
$ apt install openvswitch-switch

$ ovs-vsctl add-br br-ex

#拉取neutron源码(环境使用的是train版本)
$ pip3 install -i http://pip.yovole.tech/simple --trusted-host pip.yovole.tech  -r requirements.txt -t /usr/local/lib/python3.8/dist-packages/

$ rm -rf /usr/lib/python3/dist-packages/OpenSSL

$ pip3 install -i http://pip.yovole.tech/simple --trusted-host pip.yovole.tech --upgrade 'sqlalchemy<2.0'

$ pip uninstall setuptools

$ python3 setup.py install

$ pip3 uninstall neutron_lib

$ pip3 install -i http://pip.yovole.tech/simple --trusted-host pip.yovole.tech  -t /usr/local/lib/python3.8/dist-packages/ neutron_lib==1.29.1

#创建neutron配置文件路径，把对应配置文件都拷贝过来并修改
$ mkdir /etc/neutron

$ ls /etc/neutron
api-paste.ini ml2_conf.ini neutron.conf rootwrap.conf rootwrap.d

$ vim /lib/systemd/system/neutron-openvswitch-agent.service
[Unit]
Description=OpenStack Neutron Open vSwitch Agent
After=syslog.target network.target network.service openvswitch-switch.service
PartOf=network.service
Requires=openvswitch-switch.service
 
[Service]
Type=simple
User=neutron
PermissionsStartOnly=true
ExecStart=/usr/bin/python3.8 /usr/local/bin/neutron-openvswitch-agent --config-file /etc/neutron/neutron.conf  --config-file /etc/neutron/ml2_conf.ini  --log-file /var/log/neutron/openvswitch-agent.log
PrivateTmp=true
KillMode=process
Restart=on-failure
 
[Install]
WantedBy=multi-user.target

$ useradd neutron

$ usermod -g neutron neutron

$ mkdir /var/log/neutron

$ chown -R root:neutron /var/log/neutron

$ chmod -R 775 /var/log/neutron
 
$ chown root:neutron /etc/neutron

$ chown root:neutron /etc/neutron/ml2_conf.ini

$ chown root:neutron /etc/neutron/neutron.conf

$ mkdir /var/lib/neutron

$ chown neutron:neutron /var/lib/neutron/
 
#缺少/usr/local/bin/privsep-helper的话，从其他节点scp过来
 
$ vim /etc/sudoers.d/neutron
Defaults:neutron !requiretty
 
neutron ALL = (root) NOPASSWD: /usr/local/bin/neutron-rootwrap /etc/neutron/rootwrap.conf *
neutron ALL = (root) NOPASSWD: /usr/local/bin/neutron-rootwrap-daemon /etc/neutron/rootwrap.conf

$ systemctl start neutron-openvswitch-agent

$ systemctl enable neutron-openvswitch-agent
```
## kuryr-controller/kuryr-cni部署
```
#获取kuryr-kubernetes源码
#如果有需要就用源码中提供的dockerfile build镜像，然后上传到仓库(注意切换到开发多子网功能的分支)
$ docker build -t kuryr/controller -f controller.Dockerfile .

$ docker build -t kuryr/cni -f cni.Dockerfile .

$ ./tools/generate_k8s_resource_definitions.sh /root/deploy/kuryr-crd

$ ls /root/deploy/kuryr-crd
certificates_secret.yml  cni_ds.yml  cni_service_account.yml  config_map.yml  controller_deployment.yml  controller_service_account.yml kuryr.conf

#手动修改kuryr.conf和config_map.yml
$ vim config_map.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kuryr-config
  namespace: kube-system
data:
  kuryr.conf: |
    [DEFAULT]
    debug = true
    [kubernetes]
    api_root = https://apiserver.cluster.local:6443
    ssl_verify_server_crt = false
    token_file = /var/run/secrets/kubernetes.io/serviceaccount/token
    pod_project_driver = annotation
    namespace_project_driver = annotation
    network_policy_project_driver = annotation
    pod_subnets_driver = autoscheduler
    pod_security_groups_driver = policy
    port_debug = true
    enabled_handlers = vif,endpoints,namespace,policy,kuryrnetwork,kuryrport,kuryrnetworkpolicy,pod_label
    controller_ha = true
    controller_ha_elector_port = 16401
    [kuryr_kubernetes]
    system_namespaces = kube-system,default,calico-apiserver,calico-system,capi-kubeadm-bootstrap-system,capi-kubeadm-control-plane-system,capi-system,cert-manager,cluster-api-provider-vcluster-system,csi,dragonfly,istio-system,kube-node-lease,kube-public,nydus,tigera-operator
    none_kuryr_cnis = calico-cni
    [neutron]
    signing_dir = /var/cache/kuryr
    project_domain_name = default
    project_name = service
    user_domain_name = default
    username = neutron
    region_name = cn-east2-1a
    password = zOO825PMoowmP9zDkCWIhFX0L123xYxFoINr4t9F
    auth_url = http://ops-sh-2.yovole.yc:35357
    auth_type = password
    [neutron_defaults]
    ovs_bridge = br-int
    pod_security_groups = c8df2601-c8fc-447e-b1b6-bdf1cdc6491e
    project = b4541e058bd943458bf30ec58cb8b419
    [octavia_defaults]
    enforce_sg_rules = false
    [namespace_subnet]
    pod_subnet_pool =
    [cni_daemon]
    bind_address = 127.0.0.1:5036
    worker_num = 30
    vif_annotation_timeout = 120
    pyroute2_timeout = 60
    docker_mode = true
    netns_proc_dir = /host_proc
    [vif_plug_ovs_privileged]
    helper_command=privsep-helper
    [vif_plug_linux_bridge_privileged]
    helper_command=privsep-helper
    [pod_vif_nested]
    worker_nodes_subnets =
    [binding]
    driver = kuryr.lib.binding.drivers.vlan
    link_iface = eth0
    [oslo_concurrency]
    lock_path = /tmp/kuryr/

#修改controller_deployment.yml中的nodeSelector以及image，并添加election container支持ha
$ vim controller_deployment.yml
      nodeSelector:
        kubernetes.io/role: master
      - image: harbor.yovole.tech/kuryr/leader-elector:0.5
      - image: harbor.yovole.tech/kuryr/leader-elector:0.5
        name: leader-elector
        args:
        - "--election=kuryr-controller"
        - "--http=0.0.0.0:16401"
        - "--election-namespace=kube-system"
        - "--ttl=5s"
        ports:
        - containerPort: 16401
          protocol: TCP

#修改cni_ds.yml中的nodeSelector以及image
$ vim cni_ds.yml
        image: harbor.yovole.tech/kuryr/cni:20230614
      nodeSelector:
        kubernetes.io/role: worker

#先创建源码包下kubernetes_crds中的crd
$ kubectl apply -f kubernetes_crds/
network_attachment_definition_crd.yaml

$ kubectl apply -f kubernetes_crds/kuryr_crds/kuryrloadbalancer.yaml

$ kubectl apply -f kubernetes_crds/kuryr_crds/kuryrnetworkpolicy.yaml

$ kubectl apply -f kubernetes_crds/kuryr_crds/kuryrnetwork.yaml

$ kubectl apply -f kubernetes_crds/kuryr_crds/kuryrport.yaml

$ kubectl apply -f config_map.yml -n kube-system

$ kubectl apply -f certificates_secret.yml -n kube-system

$ kubectl apply -f controller_service_account.yml -n kube-system

$ kubectl apply -f cni_service_account.yml -n kube-system

$ kubectl apply -f controller_deployment.yml -n kube-system

$ kubectl apply -f cni_ds.yml -n kube-system
```
## multus-cni部署
### 创建kuryr/calico配置
```
$ vim calico-cni.conflist
{
  "name": "calico-cni",
  "cniVersion": "0.3.1",
  "plugins": [
    {
      "name": "k8s-pod-network",
      "cniVersion": "0.3.1",
      "type": "calico",
      "log_level": "info",
      "log_file_path": "/var/log/calico/cni/cni.log",
      "datastore_type": "kubernetes",
      "nodename_file_optional": false,
      "mtu": 0,
      "ipam": { "type": "calico-ipam", "assign_ipv4" : "true", "assign_ipv6" : "false"},
      "container_settings": {
          "allow_ip_forwarding": false
      },
      "policy": {
          "type": "k8s"
      },
      "kubernetes": {
          "k8s_api_root":"https://100.123.0.1:443",
          "kubeconfig": "/etc/cni/net.d/calico-kubeconfig"
      }
    },
    {
      "type": "portmap",
      "snat": true,
      "capabilities": {"portMappings": true}
    },
    {
      "type": "bandwidth",
      "capabilities": {"bandwidth": true}
    }
  ]
}

$ vim kuryr-cni.conflist
{
  "name": "kuryr-cni",
  "cniVersion": "0.3.1",
  "plugins": [
    {
      "type": "kuryr-cni",
      "kuryr_conf": "/etc/kuryr/kuryr.conf", # 配置文件放到对应目录
      "debug": true
    }
  ]
}
```
### 创建NetworkAttachmentDefinition
```
$ vim calico-cni.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: calico-cni
  namespace: kube-system
spec:
  config: |-
    {
      "name": "calico-cni",
      "cniVersion": "0.3.1",
      "plugins": [
      {
        "name": "k8s-pod-network",
        "cniVersion": "0.3.1",
        "type": "calico",
        "log_level": "info",
        "log_file_path": "/host/var/log/calico/cni/cni.log",
        "datastore_type": "kubernetes",
        "nodename_file_optional": false,
        "mtu": 0,
        "ipam": { "type": "calico-ipam", "assign_ipv4" : "true", "assign_ipv6" : "false"},
        "container_settings": {
          "allow_ip_forwarding": false
        },
        "policy": {
          "type": "k8s"
        },
        "kubernetes": {
          "k8s_api_root": "https://100.123.0.1:443",
          "kubeconfig": "/host/etc/cni/net.d/calico-kubeconfig"
        }
      },
      {
        "type": "portmap",
        "snat": true,
        "capabilities": {"portMappings": true}
      },
      {
        "type": "bandwidth",
        "capabilities": {"bandwidth": true}
      }
      ]
    }

$ vim kuryr-cni.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: kuryr-cni
  namespace: kube-system
spec:
  config: |-
    {
      "name": "kuryr-cni",
      "cniVersion": "0.3.1",
      "plugins": [
      {
        "type": "kuryr-cni",
        "kuryr_conf": "/etc/kuryr/kuryr.conf",
        "debug": true
      }
      ]
    }
```
### 创建multus-ds
```
$ vim multus-daemonset.yaml
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: network-attachment-definitions.k8s.cni.cncf.io
spec:
  group: k8s.cni.cncf.io
  scope: Namespaced
  names:
    plural: network-attachment-definitions
    singular: network-attachment-definition
    kind: NetworkAttachmentDefinition
    shortNames:
      - net-attach-def
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          description: 'NetworkAttachmentDefinition is a CRD schema specified by the Network Plumbing
            Working Group to express the intent for attaching pods to one or more logical or physical
            networks. More information available at: https://github.com/k8snetworkplumbingwg/multi-net-spec'
          type: object
          properties:
            apiVersion:
              description: 'APIVersion defines the versioned schema of this represen
                tation of an object. Servers should convert recognized schemas to the
                latest internal value, and may reject unrecognized values. More info:
                https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
              type: string
            kind:
              description: 'Kind is a string value representing the REST resource this
                object represents. Servers may infer this from the endpoint the client
                submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
              type: string
            metadata:
              type: object
            spec:
              description: 'NetworkAttachmentDefinition spec defines the desired state of a network attachment'
              type: object
              properties:
                config:
                  description: 'NetworkAttachmentDefinition config is a JSON-formatted CNI configuration'
                  type: string
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: multus
rules:
  - apiGroups: ["k8s.cni.cncf.io"]
    resources:
      - '*'
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/status
    verbs:
      - get
      - update
  - apiGroups:
      - ""
      - events.k8s.io
    resources:
      - events
    verbs:
      - create
      - patch
      - update
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: multus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: multus
subjects:
  - kind: ServiceAccount
    name: multus
    namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: multus
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: multus-daemon-config
  namespace: kube-system
  labels:
    tier: node
    app: multus
data:
  daemon-config.json: |
    {
        "chrootDir": "",
        "confDir": "/host/etc/cni/net.d",
        "logToStderr": true,
        "logLevel": "debug",
        "logFile": "/tmp/multus.log",
        "binDir": "/hostroot/opt/cni/bin",
        "cniDir": "/hostroot/var/lib/cni/multus",
        "socketDir": "/host/run/multus/",
        "kubeconfig": "/hostroot/etc/kubernetes/admin.conf",
        "clusterNetwork": "kuryr-cni",
        "defaultNetworks": []
    }
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-multus-ds
  namespace: kube-system
  labels:
    tier: node
    app: multus
    name: multus
spec:
  selector:
    matchLabels:
      name: multus
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        tier: node
        app: multus
        name: multus
    spec:
      nodeSelector:
        multus: "true" #把multus调度到worker节点
      hostNetwork: true
      hostPID: true
      tolerations:
        - operator: Exists
          effect: NoSchedule
        - operator: Exists
          effect: NoExecute
      serviceAccountName: multus
      containers:
        - name: kube-multus
          image: harbor.yovole.tech/vcluster/multus-cni:snapshot-thick-mutltiple-thread
          command: [ "/usr/src/multus-cni/bin/multus-daemon" ]
          args:
            - "-cni-version=0.3.1"
            - "-cni-config-dir=/host/etc/cni/net.d"
            - "-multus-autoconfig-dir=/host/etc/cni/net.d"
            - "-multus-log-to-stderr=true"
            - "-multus-log-level=verbose"
          resources:
            requests:
              cpu: "100m"
              memory: "500Mi"
            limits:
              cpu: "100m"
              memory: "500Mi"
          securityContext:
            privileged: true
          volumeMounts:
            - name: cni
              mountPath: /host/etc/cni/net.d
            - name: host-run
              mountPath: /host/run
            - name: host-var-lib-cni-multus
              mountPath: /var/lib/cni/multus
            - name: host-var-lib-kubelet
              mountPath: /var/lib/kubelet
            - name: host-run-k8s-cni-cncf-io
              mountPath: /run/k8s.cni.cncf.io
            - name: host-run-netns
              mountPath: /run/netns
              mountPropagation: HostToContainer
            - name: multus-daemon-config
              mountPath: /etc/cni/net.d/multus.d
              readOnly: true
            - name: calico-nodename
              mountPath: /var/lib/calico
              readOnly: true
            - name: hostroot
              mountPath: /hostroot
              mountPropagation: HostToContainer
      initContainers:
        - name: install-multus-binary
          image: harbor.yovole.tech/vcluster/multus-cni:snapshot-thick-mutltiple-thread
          command:
            - "cp"
            - "/usr/src/multus-cni/bin/multus-shim"
            - "/host/opt/cni/bin/multus-shim"
          resources:
            requests:
              cpu: "10m"
              memory: "15Mi"
          securityContext:
            privileged: true
          volumeMounts:
            - name: cnibin
              mountPath: /host/opt/cni/bin
              mountPropagation: Bidirectional
      terminationGracePeriodSeconds: 10
      volumes:
        - name: cni
          hostPath:
            path: /etc/cni/net.d
        - name: cnibin
          hostPath:
            path: /opt/cni/bin
        - name: hostroot
          hostPath:
            path: /
        - name: multus-daemon-config
          configMap:
            name: multus-daemon-config
            items:
            - key: daemon-config.json
              path: daemon-config.json
        - name: host-run
          hostPath:
            path: /run
        - name: host-var-lib-cni-multus
          hostPath:
            path: /var/lib/cni/multus
        - name: host-var-lib-kubelet
          hostPath:
            path: /var/lib/kubelet
        - name: host-run-k8s-cni-cncf-io
          hostPath:
            path: /run/k8s.cni.cncf.io
        - name: host-run-netns
          hostPath:
            path: /run/netns/
        - name: calico-nodename
          hostPath:
            path: /var/lib/calico
```
## 部署vcluster
### clusterctl init(需要外网)
```
#获取二进制文件
$ install -o root -g root -m 0755 clusterctl-linux-amd64 /usr/local/bin/clusterctl

$ clusterctl init --infrastructure vcluster

#init后各组件可能无法拉取镜像，修改为内部镜像，并去掉组件rollingUpdate
$ vim capi-controller-manager.yaml
$ vim capi-kubeadm-control-plane-controller-manager.yaml
$ vim cert-manager-webhook.yaml
$ vim cluster-api-provider-vcluster-controller-manager.yaml
$ vim capi-kubeadm-bootstrap-controller-manager.yaml
$ vim cert-manager-cainjector.yaml
$ vim cert-manager.yaml
```


## 部署kata

## 